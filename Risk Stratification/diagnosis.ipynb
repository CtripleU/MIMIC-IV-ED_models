{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV1WWzsndF_4",
        "outputId": "5ba3dbdd-b2e0-4720-f41a-557d2d0b718b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Set up directory paths for using Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# base_dir = \"/content/drive/MyDrive/ed/\"\n",
        "\n",
        "# # Load .csv files\n",
        "# edstays = pd.read_csv('/content/drive/MyDrive/ed/edstays.csv')\n",
        "# diagnosis = pd.read_csv('/content/drive/MyDrive/ed/diagnosis.csv')\n",
        "# triage = pd.read_csv('/content/drive/MyDrive/ed/triage.csv')\n",
        "# vitalsign = pd.read_csv('/content/drive/MyDrive/ed/vitalsign.csv')\n",
        "# medrecon = pd.read_csv('/content/drive/MyDrive/ed/medrecon.csv')\n",
        "# # pyxis = pd.read_csv('/content/drive/MyDrive/ed/pyxis.csv')\n",
        "\n",
        "\n",
        "edstays = pd.read_csv('edstays.csv')\n",
        "diagnosis = pd.read_csv('diagnosis.csv')\n",
        "triage = pd.read_csv('triage.csv')\n",
        "vitalsign = pd.read_csv('vitalsign.csv')\n",
        "medrecon = pd.read_csv('medrecon.csv')\n",
        "# pyxis = pd.read_csv('/content/drive/MyDrive/ed/pyxis.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "igu3wviMh-iN"
      },
      "outputs": [],
      "source": [
        "# Handling missing values\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "data = pd.merge(edstays, diagnosis, on=['subject_id', 'stay_id'])\n",
        "data = pd.merge(data, triage, on=['subject_id', 'stay_id'])\n",
        "data = pd.merge(data, vitalsign, on=['subject_id', 'stay_id'])\n",
        "data = pd.merge(data, medrecon, on=['subject_id', 'stay_id'])\n",
        "\n",
        "# Handling missing values\n",
        "# Using mean for numerical features and the most frequent value for categorical features\n",
        "num_imputer = SimpleImputer(strategy='mean') # Define the imputer for numerical columns\n",
        "\n",
        "# Define the numerical columns with missing values\n",
        "numerical_cols = ['temperature_x', 'heartrate_x', 'resprate_x', 'o2sat_x', 'sbp_x', 'dbp_x', 'pain_x',\n",
        "                  'temperature_y', 'heartrate_y', 'resprate_y', 'o2sat_y', 'sbp_y', 'dbp_y', 'pain_y', 'acuity']\n",
        "\n",
        "# Replace 'UA', 'Critical', 'does not scale', 'denies', and 'uncooperative' in 'pain_x' and 'pain_y' with -1\n",
        "data['pain_x'] = data['pain_x'].replace(['UA', 'Critical', 'does not scale', 'denies', 'uncooperative'], -1)\n",
        "data['pain_y'] = data['pain_y'].replace(['UA', 'does not scale', 'denies', 'uncooperative'], -1)\n",
        "\n",
        "# data['pain_x'] = pd.to_numeric(data['pain_x'], errors='coerce').fillna(pd.to_numeric(data['pain_x'], errors='coerce').mean())\n",
        "# data['pain_y'] = pd.to_numeric(data['pain_y'], errors='coerce').fillna(pd.to_numeric(data['pain_y'], errors='coerce').mean())\n",
        "\n",
        "\n",
        "# Fill missing values in numerical columns with their mean\n",
        "data[numerical_cols] = num_imputer.fit_transform(data[numerical_cols])\n",
        "\n",
        "# # Fill missing values in 'hadm_id' with -1\n",
        "# data['hadm_id'] = data['hadm_id'].fillna(-1)\n",
        "\n",
        "# Fill missing values in 'hadm_id' with 0\n",
        "data['hadm_id'] = data['hadm_id'].fillna(0)\n",
        "\n",
        "data[numerical_cols] = num_imputer.fit_transform(data[numerical_cols])\n",
        "# data[categorical_cols] = cat_imputer.fit_transform(data[categorical_cols])\n",
        "\n",
        "# Fill missing values in 'acuity' with its mode\n",
        "data['acuity'] = data['acuity'].fillna(data['acuity'].mode()[0])\n",
        "\n",
        "# Fill missing values in 'chiefcomplaint' with 'missing'\n",
        "data['chiefcomplaint'] = data['chiefcomplaint'].fillna('missing')\n",
        "\n",
        "# Fill missing values in 'rhythm' with 'missing'\n",
        "data['rhythm'] = data['rhythm'].fillna('missing')\n",
        "\n",
        "# # Fill missing values in 'rhythm' with its mode\n",
        "# data['rhythm'] = data['rhythm'].fillna(data['rhythm'].mode()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jSM-0s4RoX6B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import scipy\n",
        "\n",
        "# print(scipy.__version__)\n",
        "# from scipy.sparse import hstack\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_selection import SelectKBest, chi2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l4-5pdsqueIL"
      },
      "outputs": [],
      "source": [
        "# Preprocess text data\n",
        "# text_cols = ['chiefcomplaint', 'icd_title','rhythm', 'name', 'race', 'arrival_transport', 'disposition']\n",
        "text_cols = ['chiefcomplaint','rhythm', 'name', 'race', 'arrival_transport', 'disposition']\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "text_data = data[text_cols].fillna('').apply(lambda x: ' '.join(x), axis=1)\n",
        "text_features = tfidf_vectorizer.fit_transform(text_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YK_4YS3pg8SN"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "text_cols = ['chiefcomplaint','rhythm', 'name', 'race', 'arrival_transport', 'disposition']\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "text_data = data[text_cols].fillna('').apply(lambda x: ' '.join(x), axis=1)\n",
        "\n",
        "count_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Consider n-grams\n",
        "text_counts = count_vectorizer.fit_transform(text_data)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "text_features = tfidf_transformer.fit_transform(text_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Aor4eApJuu6q"
      },
      "outputs": [],
      "source": [
        "# Encode categorical data\n",
        "categorical_cols = ['icd_code']\n",
        "label_encoder = LabelEncoder()\n",
        "data[categorical_cols] = data[categorical_cols].apply(lambda x: label_encoder.fit_transform(x))\n",
        "\n",
        "# One-hot encode remaining categorical columns\n",
        "categorical_cols = ['gender', 'race', 'arrival_transport', 'disposition']\n",
        "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "categorical_data = one_hot_encoder.fit_transform(data[categorical_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PRbAbaD0h5Vq"
      },
      "outputs": [],
      "source": [
        "# Extract date-time features\n",
        "data['intime_hour'] = pd.to_datetime(data['intime']).dt.hour\n",
        "data['intime_day'] = pd.to_datetime(data['intime']).dt.day\n",
        "data['intime_month'] = pd.to_datetime(data['intime']).dt.month\n",
        "data['intime_year'] = pd.to_datetime(data['intime']).dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MK9l_-Ne1Llx"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import hstack\n",
        "\n",
        "# Convert DataFrames to sparse matrices\n",
        "numerical_data = scipy.sparse.csr_matrix(data[numerical_cols].values)\n",
        "datetime_data = scipy.sparse.csr_matrix(data[['intime_hour', 'intime_day', 'intime_month', 'intime_year']].values)\n",
        "\n",
        "# Stack all features\n",
        "# X = hstack([numerical_data, text_features, categorical_data, datetime_data])\n",
        "X = hstack([numerical_data, text_features, categorical_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7PV5AUgSiAmc"
      },
      "outputs": [],
      "source": [
        "# Prepare target variable\n",
        "data = data.explode('icd_code')\n",
        "data['diagnosis'] = data.apply(lambda x: str(x['icd_code']) + ' ' + str(x['icd_title']), axis=1)\n",
        "y = data['diagnosis']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wHzNIWiAh-yG"
      },
      "outputs": [],
      "source": [
        "# # Perform feature selection using chi-squared test\n",
        "# selector = SelectKBest(chi2, k=10)\n",
        "# X_selected = selector.fit_transform(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yP06oSIGiDJd"
      },
      "outputs": [],
      "source": [
        "# Split data into train and test sets\n",
        "# Split the data into training and a temporary set using an 80/20 split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the temporary set into validation and test sets using a 50/50 split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "y_train = y_train.values.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LFUrg49jh1AI",
        "outputId": "10397c95-16ee-4dbe-d785-08506157ec57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average F1-score: 0.7482326286599884\n",
            "Average Accuracy: 0.7614920670777658\n",
            "Average Precision: 0.7682211381138798\n",
            "Average Recall: 0.7614920301656758\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean([score[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mscore\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mscores])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean([score[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mscore\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mscores])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage ROC AUC Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(\u001b[43m[\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mscore\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[16], line 36\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean([score[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mscore\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mscores])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean([score[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mscore\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mscores])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage ROC AUC Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean([\u001b[43mscore\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mscore\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mscores])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ],
      "source": [
        "# Class Imbalance Handling\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# Ensemble Method\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Cross-Validation\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "for train_idx, val_idx in skf.split(X_resampled, y_resampled):\n",
        "    X_train_cv, X_val_cv = X_resampled[train_idx], X_resampled[val_idx]\n",
        "    y_train_cv, y_val_cv = y_resampled[train_idx], y_resampled[val_idx]\n",
        "\n",
        "    model.fit(X_train_cv, y_train_cv)\n",
        "    y_pred = model.predict(X_val_cv)\n",
        "\n",
        "    f1 = f1_score(y_val_cv, y_pred, average='macro')\n",
        "    acc = accuracy_score(y_val_cv, y_pred)\n",
        "    precision = precision_score(y_val_cv, y_pred, average='macro')\n",
        "    recall = recall_score(y_val_cv, y_pred, average='macro')\n",
        "    \n",
        "    scores.append((f1, acc, precision, recall))\n",
        "\n",
        "print(f\"Average F1-score: {np.mean([score[0] for score in scores])}\")\n",
        "print(f\"Average Accuracy: {np.mean([score[1] for score in scores])}\")\n",
        "print(f\"Average Precision: {np.mean([score[2] for score in scores])}\")\n",
        "print(f\"Average Recall: {np.mean([score[3] for score in scores])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuLs4h6dw7yT"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "import pickle\n",
        "pickle.dump(model, open('diagnosis_prediction_model.pkl', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
